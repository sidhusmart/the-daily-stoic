{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the stoic quote, author, reflection for each day from the PDF document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Make use of an LLM to extract the date, quote, author, reflection from each page of the PDF using an LLM\n",
    "\n",
    "**Why**: We can use PDF reader libraries but it might be harder for them to understand the layout and distinction between the 4 distinct elements given that they typically access the text directly. It might mean adding some weird heuristics or regex patterns to enable this flow. Hence LLMs. \n",
    "\n",
    "**How**: Single PDF Page -> Image -> LLM -> structured output\n",
    "\n",
    "**Notes**: Additionally, we will try to use the OpenAI Batch API just to test it and compare cost savings (if any). The methodology to do it is different so might as well give it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Path to your PDF\n",
    "pdf_path = \"assets/The_Daily_Stoic_Book.pdf\"\n",
    "\n",
    "# Convert all pages to images\n",
    "images = convert_from_path(pdf_path, dpi=300)  # dpi=300 for good quality\n",
    "\n",
    "# Save each page as an image\n",
    "for i, image in enumerate(images):\n",
    "    image.save(f\"assets/page_{i+1}.png\", \"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the prompt first and with an example.\n",
    "\n",
    "We have extracted the individual pages as images now, let's create the prompt. We will use a one-shot example to guide the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up the OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"05-16\": {\n",
      "    \"title\": \"THE CHAIN METHOD\",\n",
      "    \"quote\": \"“If you don’t wish to be a hot-head, don’t feed your habit. Try as a first step to remain calm and count the days you haven’t been angry. I used to be angry every day, now every other day, then every third or fourth . . . if you make it as far as 30 days, thank God! For habit is first weakened and then obliterated. When you can say ‘I didn’t lose my temper today, or the next day, or for three or four months, but kept my cool under provocation,’ you will know you are in better health.”\",\n",
      "    \"author\": \"EPICTETUS, DISCOURSES, 2.18.11b-14\",\n",
      "    \"reflection\": \"The comedian Jerry Seinfeld once gave a young comic named Brad Isaac some advice about how to write and create material. Keep a calendar, he told him, and each day that you write jokes, put an X. Soon enough, you get a chain going—and then your job is to simply not break the chain. Success becomes a matter of momentum. Once you get a little, it’s easier to keep it going. Whereas Seinfeld used the chain method to build a positive habit, Epictetus was saying that it can also be used to eliminate a negative one. It’s not all that different than taking sobriety “one day at a time.” Start with one day doing whatever it is, be it managing your temper or wandering eyes or procrastination. Then do the same the following day and the day after that. Build a chain and then work not to break it. Don’t ruin your streak.\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Encode images as base64\n",
    "example_image_b64 = encode_image(\"assets/page_152.png\")\n",
    "target_image_b64 = encode_image(\"assets/page_156.png\")\n",
    "\n",
    "# Your hand-crafted JSON for the example\n",
    "example_json = \"\"\"\n",
    "{\n",
    "  \"05-12\": {\n",
    "    \"title\": \"KINDNESS IS ALWAYS THE RIGHT RESPONSE\",\n",
    "    \"quote\": \"“Kindness is invincible, but only when it's sincere, with no hypocricy or faking. For what can even the most malicious person do if you keep showing kindness and, if given the chance, you gently point out where they went wrong - right as they are trying to harm you?”\",\n",
    "    \"author\": \"MARCUS AURELIUS, MEDITATIONS, 11.18.5.9a\",\n",
    "    \"reflection\": \"What if the next time you were treated meanly you didn't just restrain yourself from fighting back - what if you responded with unmitigated kindness? What if you could \\\"love your enemies, do good to those who hate you\\\"? What kind of effect do you think that would have? \\nThe Bible says that when you can do something nice and caring to a hateful enemy, it is like \\\"heap[ing] burning coal on his head.\\\" The expected reaction to hatred is more hatred. When someone says something pointed or mean today, they expect you to respond in kind - not with kindness. When that doesn't happen, they are embarrassed. It's a shock to their system - it makes them and you better. \\nMost rudeness, meanness, and cruelty are a mask for deep-seated weakness. Kindness in these situations is only possible for people of great strength. You have that strength. Use it.\"\n",
    "  },\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Compose the messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that extracts structured data from images of book pages.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Here is an example image of a Stoic quote page:\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{example_image_b64}\"}},\n",
    "            {\"type\": \"text\", \"text\": f\"The extracted JSON for this page is:\\n{example_json}\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Now, extract the JSON for this new image, following the same format if it is a Stoic quote page. If the image is not a quote page (for example, a title or copyright page), please return the string SKIP.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{target_image_b64}\"}}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the API\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # or \"gpt-4o-mini\" if available\n",
    "    messages=messages,\n",
    "    max_tokens=512,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "response_text = response.choices[0].message.content\n",
    "\n",
    "# This regex finds the content between ```json ... ```\n",
    "match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", response_text, re.DOTALL)\n",
    "if match:\n",
    "    json_str = match.group(1)\n",
    "else:\n",
    "    # Fallback: try to find any JSON object in the text\n",
    "    match = re.search(r\"(\\{.*\\})\", response_text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        raise ValueError(\"No JSON object found in the response.\")\n",
    "\n",
    "stoic_quote = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'05-16': {'title': 'THE CHAIN METHOD',\n",
       "  'quote': '“If you don’t wish to be a hot-head, don’t feed your habit. Try as a first step to remain calm and count the days you haven’t been angry. I used to be angry every day, now every other day, then every third or fourth . . . if you make it as far as 30 days, thank God! For habit is first weakened and then obliterated. When you can say ‘I didn’t lose my temper today, or the next day, or for three or four months, but kept my cool under provocation,’ you will know you are in better health.”',\n",
       "  'author': 'EPICTETUS, DISCOURSES, 2.18.11b-14',\n",
       "  'reflection': 'The comedian Jerry Seinfeld once gave a young comic named Brad Isaac some advice about how to write and create material. Keep a calendar, he told him, and each day that you write jokes, put an X. Soon enough, you get a chain going—and then your job is to simply not break the chain. Success becomes a matter of momentum. Once you get a little, it’s easier to keep it going. Whereas Seinfeld used the chain method to build a positive habit, Epictetus was saying that it can also be used to eliminate a negative one. It’s not all that different than taking sobriety “one day at a time.” Start with one day doing whatever it is, be it managing your temper or wandering eyes or procrastination. Then do the same the following day and the day after that. Build a chain and then work not to break it. Don’t ruin your streak.'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoic_quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create the batch JSON file for the OpenAI Batch job\n",
    "\n",
    "We are following the instructions provided on this page - https://platform.openai.com/docs/guides/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSONL file created: assets/stoic_requests.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "\n",
    "# Path to the folder with target images (images to process)\n",
    "target_image_folder = \"assets\"\n",
    "\n",
    "# Path to a fixed example image (used in every request)\n",
    "example_image_path = \"assets/page_152.png\"\n",
    "\n",
    "# Path to save the output .jsonl file\n",
    "output_jsonl_path = \"assets/stoic_requests.jsonl\"\n",
    "\n",
    "# The example JSON string (used in prompt)\n",
    "example_json = \"\"\"\n",
    "{\n",
    "  \"05-12\": {\n",
    "    \"title\": \"KINDNESS IS ALWAYS THE RIGHT RESPONSE\",\n",
    "    \"quote\": \"“Kindness is invincible, but only when it's sincere, with no hypocricy or faking. For what can even the most malicious person do if you keep showing kindness and, if given the chance, you gently point out where they went wrong - right as they are trying to harm you?”\",\n",
    "    \"author\": \"MARCUS AURELIUS, MEDITATIONS, 11.18.5.9a\",\n",
    "    \"reflection\": \"What if the next time you were treated meanly you didn't just restrain yourself from fighting back - what if you responded with unmitigated kindness? What if you could \\\"love your enemies, do good to those who hate you\\\"? What kind of effect do you think that would have? \\nThe Bible says that when you can do something nice and caring to a hateful enemy, it is like \\\"heap[ing] burning coal on his head.\\\" The expected reaction to hatred is more hatred. When someone says something pointed or mean today, they expect you to respond in kind - not with kindness. When that doesn't happen, they are embarrassed. It's a shock to their system - it makes them and you better. \\nMost rudeness, meanness, and cruelty are a mask for deep-seated weakness. Kindness in these situations is only possible for people of great strength. You have that strength. Use it.\"\n",
    "  },\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# === HELPER FUNCTIONS ===\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# === MAIN PROCESSING ===\n",
    "\n",
    "# Encode the static example image\n",
    "example_image_b64 = encode_image_to_base64(example_image_path)\n",
    "\n",
    "# Open the output file\n",
    "with open(output_jsonl_path, \"w\") as outfile:\n",
    "    for idx, filename in enumerate(sorted(os.listdir(target_image_folder))):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            image_path = os.path.join(target_image_folder, filename)\n",
    "            target_image_b64 = encode_image_to_base64(image_path)\n",
    "\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant that extracts structured data from images of book pages.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Here is an example image of a Stoic quote page:\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{example_image_b64}\"}},\n",
    "                        {\"type\": \"text\", \"text\": f\"The extracted JSON for this page is:\\n{example_json}\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Now, extract the JSON for this new image, following the same format if it is a Stoic quote page. If the image is not a quote page (for example, a title or copyright page), please return the string SKIP.\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{target_image_b64}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            request_payload = {\n",
    "                \"custom_id\": f\"request-{idx+1}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"messages\": messages,\n",
    "                    \"max_tokens\": 512,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "\n",
    "            outfile.write(json.dumps(request_payload) + \"\\n\")\n",
    "\n",
    "print(f\"✅ JSONL file created: {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File size issue: the Batch API only allows a maximum file size of 200MB. \n",
    "\n",
    "Because of the images that we are using, this was much larger than that (320MB) so we will split it into two files and try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stoic_requests_part_1.jsonl\n",
      "Wrote stoic_requests_part_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_file = \"assets/stoic_requests.jsonl\"\n",
    "max_lines_per_file = 220  # adjust based on average size per line\n",
    "\n",
    "with open(input_file, 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "for i in range(0, len(lines), max_lines_per_file):\n",
    "    chunk = lines[i:i+max_lines_per_file]\n",
    "    chunk_filename = f\"stoic_requests_part_{i//max_lines_per_file + 1}.jsonl\"\n",
    "    with open(chunk_filename, 'w') as outfile:\n",
    "        outfile.writelines(chunk)\n",
    "    print(f\"Wrote {chunk_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-NAfWRkN2HVwcj61RX2ivAE', bytes=150049667, created_at=1747670171, filename='stoic_requests_part_2.jsonl', object='file', purpose='batch', status='processed', status_details=None, expires_at=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(\"assets/stoic_requests_part_2.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(batch_input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our fileID is: file-N9h3oEKMbgVkbtqHNPuGuJ [part1] and fileID is: file-NAfWRkN2HVwcj61RX2ivAE [part2]\n",
    "- We have to use that when creating the batch request job\n",
    "- So far, the cost estimate for three requests seems to be roughly 2 cents. If we extract that to 400 pages (400 requests), I expect it to be between 2 and 3 USD. The batch API promises a 50% discount as we are waiting - so the costs should be around a dollar. We have to wait and watch!\n",
    "- **Final result** - there was a bit of an additional hoop around splitting the file but the overall cost came to 1.44 USD - right in the range of what we had predicted! Makes good sense to use the Batch API.\n",
    "- The response was also quite fast - I mean it's a batch job so not immediate but within 10 minutes both batches were done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_682b54bae3e88190827800445fae2b47', completion_window='24h', created_at=1747670202, endpoint='/v1/chat/completions', input_file_id='file-NAfWRkN2HVwcj61RX2ivAE', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747756602, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'stoic extraction job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"stoic extraction job\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Status Check\n",
    "\n",
    "Now, let's check the status. The same code can be executed at any time to keep track.\n",
    "\n",
    "- 1st batch request: batch_682b5019b17c8190808a26d7fede9ba5\n",
    "- 2nd batch request: batch_682b54bae3e88190827800445fae2b47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id_number = 'batch_682b54bae3e88190827800445fae2b47'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_682b54bae3e88190827800445fae2b47', completion_window='24h', created_at=1747670202, endpoint='/v1/chat/completions', input_file_id='file-NAfWRkN2HVwcj61RX2ivAE', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747670501, error_file_id=None, errors=None, expired_at=None, expires_at=1747756602, failed_at=None, finalizing_at=1747670486, in_progress_at=1747670271, metadata={'description': 'stoic extraction job'}, output_file_id='file-E6Q1YS8jaSTkdD4aX9BXfn', request_counts=BatchRequestCounts(completed=187, failed=0, total=187))\n"
     ]
    }
   ],
   "source": [
    "batch = client.batches.retrieve(batch_id_number)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve batch job\n",
    "\n",
    "Once the job has completed, you will get an output_file_id field. You need to use this field to retrieve the output.\n",
    "\n",
    "- Part1 output_file_id=file-5bVxF77RWpnEzgRAAQpvx4\n",
    "- Part2 output_file_id=file-E6Q1YS8jaSTkdD4aX9BXfn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output saved to assets/stoic_results_part_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "output_file_id='file-E6Q1YS8jaSTkdD4aX9BXfn'\n",
    "\n",
    "file_response = client.files.content(output_file_id)\n",
    "\n",
    "output_path = \"assets/stoic_results_part_2.jsonl\"\n",
    "\n",
    "# Save the streamed content into a local file\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(file_response.read())\n",
    "\n",
    "print(f\"✅ Output saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results\n",
    "\n",
    "Read in the results for a particular day (row) and see how the response looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"11-26\": {\n",
      "    \"title\": \"THE ALTAR OF NO DIFFERENCE\",\n",
      "    \"quote\": \"“We are like many pellets of incense falling on the same altar. Some collapse sooner, others later, but it makes no difference.”\",\n",
      "    \"author\": \"MARCUS AURELIUS, MEDITATIONS, 4.15\",\n",
      "    \"reflection\": \"What's the difference between you and the richest person in the world? One has a little more money than the other. What's the difference between you and the oldest person in the world? One has been around a little longer than the other. Same goes for the tallest, smartest, fastest, and on down the line. Measuring ourselves against other people makes acceptance difficult, because we want what they have, or we want how things could have gone, not what we happen to have. But that makes no difference. Some might see this line from Marcus as pessimistic, whereas others see it as optimistic. It's really just truth. We're all here and we're all going to leave this earth eventually, so let's not concern ourselves with petty differences in the meantime. We have too much to do.\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# Path to your jsonl file\n",
    "jsonl_file_path = \"assets/stoic_results_part_2.jsonl\"\n",
    "\n",
    "# Step 1: Load all lines\n",
    "with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Step 2: Pick a random line\n",
    "random_line = random.choice(lines)\n",
    "\n",
    "# Step 3: Parse the JSON line\n",
    "response_obj = json.loads(random_line)\n",
    "\n",
    "# Step 4: Inspect the object\n",
    "quote_json = response_obj[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "print(quote_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together into a single JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 367 quotes from 2 files.\n",
      "⚠️ Skipped 40 lines.\n",
      "\n",
      "Details of skipped lines:\n",
      "assets/stoic_results_part_1.jsonl — Line 1: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 2: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 11: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 13: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 24: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 35: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 45: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 46: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 47: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 82: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 112: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 117: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 152: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 187: SKIP\n",
      "assets/stoic_results_part_1.jsonl — Line 188: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 2: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 3: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 39: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 73: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 108: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 109: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 111: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 112: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 113: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 114: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 116: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 117: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 118: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 119: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 120: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 121: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 122: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 128: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 132: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 143: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 154: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 161: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 165: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 176: SKIP\n",
      "assets/stoic_results_part_2.jsonl — Line 187: SKIP\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# List of input .jsonl files\n",
    "input_files = [\n",
    "    \"assets/stoic_results_part_1.jsonl\",\n",
    "    \"assets/stoic_results_part_2.jsonl\"\n",
    "]\n",
    "\n",
    "# Output file\n",
    "output_path = \"stoic_quotes.json\"\n",
    "\n",
    "valid_quotes = []\n",
    "skipped_lines = []\n",
    "\n",
    "for input_path in input_files:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f, start=1):\n",
    "            try:\n",
    "                response_obj = json.loads(line)\n",
    "                content = response_obj[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "                # Strip triple backticks and possible `json` marker\n",
    "                if content.startswith(\"```json\"):\n",
    "                    content = content.removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "                elif content.startswith(\"```\"):\n",
    "                    content = content.removeprefix(\"```\").removesuffix(\"```\").strip()\n",
    "\n",
    "                if content == \"SKIP\":\n",
    "                    skipped_lines.append(f\"{input_path} — Line {idx}: SKIP\")\n",
    "                    continue\n",
    "\n",
    "                # Parse the inner string as JSON\n",
    "                quote_data = json.loads(content)\n",
    "                valid_quotes.append(quote_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                skipped_lines.append(f\"{input_path} — Line {idx}: Error - {str(e)}\")\n",
    "\n",
    "# Save all valid JSONs into one unified JSON file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    json.dump(valid_quotes, out_f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Summary\n",
    "print(f\"✅ Extracted {len(valid_quotes)} quotes from {len(input_files)} files.\")\n",
    "print(f\"⚠️ Skipped {len(skipped_lines)} lines.\")\n",
    "if skipped_lines:\n",
    "    print(\"\\nDetails of skipped lines:\")\n",
    "    for line in skipped_lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Exactly 367 quotes have been extracted -> that's like icing on the cake. No error as such!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
